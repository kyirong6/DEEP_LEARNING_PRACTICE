{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A SIMPLE EXAMPLE OF A FEED FOWARD NN USING THE MNIST DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This guide is built upon a tutorial from the youtube channel: sentdex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data used to train the model\n",
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# data used to test the model \n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**batches:** batches of data dictates how many samples we want to feed into the model per update.\n",
    "we don't want to use the whole data set because this can become too much for the computer to handle.\n",
    "batches also help with generalization of the model. (usually between 8-64)\n",
    "\n",
    "**batch size:** a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "\n",
    "**shuffle:** helps with generalization. Randomizes the order of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10,shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([4, 2, 4, 8, 8, 8, 2, 6, 6, 3])]\n"
     ]
    }
   ],
   "source": [
    "# iterating over the data:\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "# accessing first input and corresponding correct label\n",
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# view() is similar to reshape\n",
    "# show the image of that data as a 28x28\n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example to show that pytorch shaping may not be intuitive\n",
    "\n",
    "the 1 in the beginning is for pytorch. The image itself should only be 28x28\n",
    "\n",
    "print(data[0][0].shape): torch.Size([1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "# balancing our data: what if our dataset was just 80% 3's? Our model would learn the number 3 really well.\n",
    "# this is not general. \n",
    "# this code snippet counts the occurances of each number.\n",
    "total = 0\n",
    "counter_dict = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total+=1\n",
    "        \n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "# the number 1 is the most popular.\n",
    "# this is fine.\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the model:\n",
    "\n",
    "**torch.nn** is a class. OOP\n",
    "\n",
    "**functional** is functions to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # inheriting from nn.Module\n",
    "        # super corresponds to nn.Module. So, it runs initializations for nn.Module\n",
    "        # if super was not there, the initializations would not run for nn.Module\n",
    "        super().__init__()\n",
    "        \n",
    "        # define fully connected layers:\n",
    "        # fc1: fully connected layer 1\n",
    "        # 1st parameter (input for layer 1): 28*28. This is the flattened input data. (28 by 28 pixels for an image)\n",
    "        # 2nd parameter (output for layer 1): 64. Could be whatever we want.  \n",
    "        # nn.Linear(): used for a simple linear network.\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        \n",
    "        # output layer: 10 classes because numbers are from 0-9\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        \n",
    "    # a function to define how the data flows through layers.\n",
    "    # for feed forward NN:\n",
    "    # x: input\n",
    "    # pytorch allows us to define how our data flows through layers however we want. \n",
    "    # we could do things that arent necessarily logical. Moreover, this allows for flexibility.\n",
    "    # however, we are going to keep things simple and standard.\n",
    "    def forward(self, x):\n",
    "        # F.relu() (rectified linear unit): our activation function we will use \n",
    "        # activation functions: a non linear function that represents a neuron \"firing\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # we dont want to pass the data from the output layer into the ReLU.\n",
    "        # we want our output to be a probability distribution of the classes.\n",
    "        # ie. we will use softmax. \n",
    "        # [.1 .3 .01 ... etc]\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        # the softmax function\n",
    "        # we must define the dimension. this is similar to the axis we want to use softmax on.\n",
    "        # dim=1 is so we distribute softmax for the outputs\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it on some data: torch.rand((28,28)): random values for a matrix of size 28x28\n",
    "X = torch.rand((28,28))\n",
    "\n",
    "# technically, we could pass this to the network.\n",
    "# output = net(X)\n",
    "# this produces a size mismatch error. X needs to flattened first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.view(28*28)\n",
    "# output = net(X)\n",
    "# this also produces an error\n",
    "# we must format things exactly how pytorch wants them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3960, -2.3709, -2.3633, -2.2998, -2.1570, -2.2333, -2.3257, -2.2636,\n",
       "         -2.3018, -2.3380]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch wants us to use -1 first to represent \"any size\". \n",
    "X = X.view(-1, 28*28)\n",
    "output = net(X)\n",
    "# output represents the actual predictions\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have not adjusted weights or done back prop. We have only passed data through the network.\n",
    "\n",
    "we need a *loss function* and an *optimizer*\n",
    "\n",
    "**loss function:** a way to calculate how wrong (error) our prediction was based on the correct answer.\n",
    "\n",
    "**optimizer:** using our calculated loss to adjust the weights in order to lower our loss over time.\n",
    "\n",
    "**gradient descent** is commonly used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0858, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Adam has 2 parameters:\n",
    "# 1: net.parameters(): everything that is adjustable/we want to be adjustable.\n",
    "# 2: learning rate: a hyperparameter that determines step size for updating weights from gradient descent\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# epoch: a full pass through of our data\n",
    "# 3: we will go through our whole data set 3 times.\n",
    "# remember, we go through our data in determined batch sizes.\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of featuresets and labels\n",
    "        # X are the features, y is the correct label\n",
    "        X, y = data\n",
    "        \n",
    "        # make sure our gradients are cleared to 0.\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # our predictions\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        # caclulate our loss. There's many ways to do this:\n",
    "        # nll_loss is used because our correct output labels is just a scalar value:\n",
    "        # ie. tensor(4)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        \n",
    "        # backprop:\n",
    "        # pytorch does this very easily for us. \n",
    "        loss.backward()\n",
    "        \n",
    "        # adjust the weights\n",
    "        optimizer.step()\n",
    "    # lets take a look at our loss decline\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.968\n"
     ]
    }
   ],
   "source": [
    "# lets see how well our model performs\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# when testing, we dont want to calculate gradients.\n",
    "with torch.no_grad():\n",
    "    # use the test data\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        \n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# viewing out input for fun\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[1].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "# seeing if our model predicted correctly\n",
    "print(torch.argmax(net(X[1].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential()\n",
    "An alternative approach for implementing a pytorch NN with a simple feedforward architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(784, 64)),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(64, 64)),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('fc3', nn.Linear(64, 64)),\n",
    "                      ('relu3', nn.ReLU()),\n",
    "                      ('output', nn.Linear(64, 10)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be used with: model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
